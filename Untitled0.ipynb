{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Untitled0.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "[View in Colaboratory](https://colab.research.google.com/github/SAKURA-AYANE/Djent_Learning/blob/master/Untitled0.ipynb)"
      ]
    },
    {
      "metadata": {
        "id": "dehM4GAyYufB",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "!pip install PyDrive"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "LML3pHBU59dq",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "  !apt-get install -y -qq software-properties-common python-software-properties module-init-tools\n",
        "  !add-apt-repository -y ppa:alessandro-strada/ppa 2>&1 > /dev/null\n",
        "  !apt-get update -qq 2>&1 > /dev/null\n",
        "  !apt-get -y install -qq google-drive-ocamlfuse fuse\n",
        "  from google.colab import auth\n",
        "  auth.authenticate_user()\n",
        "  from oauth2client.client import GoogleCredentials\n",
        "  creds = GoogleCredentials.get_application_default()\n",
        "  import getpass\n",
        "  !google-drive-ocamlfuse -headless -id={creds.client_id} -secret={creds.client_secret} < /dev/null 2>&1 | grep URL\n",
        "  vcode = getpass.getpass()\n",
        "  !echo {vcode} | google-drive-ocamlfuse -headless -id={creds.client_id} -secret={creds.client_secret}\n",
        "\n",
        "  !mkdir -p drive\n",
        "  !google-drive-ocamlfuse drive"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "bSxl2-Sm8dAu",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "  !mkdir -p drive\n",
        "  !google-drive-ocamlfuse drive"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "jbJeuShs130i",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 374
        },
        "outputId": "d8daf67d-d808-4208-d272-f0e21e47e546"
      },
      "cell_type": "code",
      "source": [
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, Dropout, LeakyReLU, LSTM, SimpleRNN\n",
        "from keras.optimizers import Adadelta\n",
        "from keras.callbacks import ModelCheckpoint\n",
        "import numpy as np\n",
        "import os\n",
        "import gc\n",
        "import errno\n",
        "from numpy import newaxis\n",
        "from scipy.io import wavfile\n",
        "import scipy.signal as signal\n",
        "rate = None\n",
        "length = None\n",
        "\n",
        "\n",
        "# file path\n",
        "model_path = \"model/lstm_djent_model.h5\"\n",
        "weight_path = \"model/weights_djent.best.hdf5\"\n",
        "expected_path = \"train/expected/djent_\"\n",
        "input_path = \"train/input/\"\n",
        "npers = 2000\n",
        "\n",
        "\n",
        "# read input file\n",
        "def get_data(path):\n",
        "    global rate\n",
        "    rate, data = wavfile.read(path)\n",
        "    global length\n",
        "    length = int(len(data) / rate) * rate  # adjust the length of data\n",
        "    data = data[0:length]  # since fft can only read n*frame_rate\n",
        "    return data\n",
        "\n",
        "\n",
        "def get_stft(path):\n",
        "    data = get_data(path)\n",
        "    _, _, stft = signal.stft(data, rate, nperseg=npers)\n",
        "    del data  # clean memory\n",
        "    stft = stft.T\n",
        "    return stft\n",
        "\n",
        "\n",
        "def stft_ri(path):\n",
        "    stft = get_stft(path)\n",
        "    real = stft.real\n",
        "    imag = stft.imag\n",
        "    del stft  # clean memory\n",
        "    ri = np.concatenate((real, imag), axis=1)\n",
        "    del real, imag  # clean memory\n",
        "    return ri\n",
        "\n",
        "\n",
        "# transfer to array for training\n",
        "def array(num):\n",
        "    dim = npers + 2\n",
        "    # create empty training arrays\n",
        "    train_in = np.empty([1, 1, dim])\n",
        "    train_out = np.empty([1, dim])\n",
        "\n",
        "    # read all audio files to one array\n",
        "    try:\n",
        "\n",
        "        # read file\n",
        "        read_in_stft = stft_ri(input_path + \"new_clean_\" + str(num) + \".wav\")\n",
        "        read_out_stft = stft_ri(expected_path + str(num)+\".wav\")\n",
        "\n",
        "        # shuffle before process\n",
        "        index = np.arange(len(read_in_stft))\n",
        "        np.random.shuffle(index)\n",
        "        read_in_stft = read_in_stft[index]\n",
        "        read_out_stft = read_out_stft[index]\n",
        "\n",
        "        # reshape\n",
        "        read_in = np.array(read_in_stft)\n",
        "        del read_in_stft   # clean memory\n",
        "        read_in = read_in[:, newaxis, :]\n",
        "        if not train_in.size:  # the first array\n",
        "            train_in = read_in\n",
        "            train_out = read_out_stft\n",
        "        else:\n",
        "            train_in = np.concatenate((train_in, read_in))\n",
        "            train_out = np.concatenate((train_out, read_out_stft))\n",
        "            del read_in, read_out_stft  # clean memory\n",
        "\n",
        "    # catch errors\n",
        "    except IOError as exc:\n",
        "        if exc.errno != errno.EISDIR:\n",
        "            raise\n",
        "    gc.collect()  # clean memory\n",
        "    return train_in, train_out\n",
        "\n",
        "dim = npers + 2  # nperseg of stft + 2\n",
        "\n",
        "# activate a new model\n",
        "model = Sequential()\n",
        "\n",
        "# input shape needs to be changed to (2, ) if using stereo audio\n",
        "model.add(LSTM(dim, return_sequences=True, input_shape=(1, dim)))\n",
        "model.add(LeakyReLU())\n",
        "model.add(Dropout(0.2))\n",
        "model.add(LSTM(dim, return_sequences=True))\n",
        "model.add(LeakyReLU())\n",
        "model.add(Dropout(0.2))\n",
        "model.add(LSTM(dim, return_sequences=False))\n",
        "model.add(LeakyReLU())\n",
        "model.add(Dropout(0.2))\n",
        "model.add(Dense(dim))\n",
        "\n",
        "''' trying with RNN model\n",
        "model.add(SimpleRNN(units=dim, input_shape=(1, dim), activation='relu', return_sequences=True, return_state=False))\n",
        "model.add(Dropout(0.1))\n",
        "model.add(SimpleRNN(units=dim, activation='relu', return_sequences=True, return_state=False))  # able to add more layers\n",
        "model.add(Dropout(0.1))  # by repeating these two lines\n",
        "model.add(SimpleRNN(units=dim, activation='relu', return_sequences=False, return_state=False))\n",
        "model.add(Dropout(0.1))\n",
        "model.add(Dense(units=dim))\n",
        "'''\n",
        "\n",
        "'''\n",
        "# control the size of fit generator\n",
        "def data_generator(data, targets, batch_size):\n",
        "    batches = len(data) + batch_size - 1\n",
        "    while(True):\n",
        "        for i in range(batches):\n",
        "            x = data[i * batch_size: (i + 1) * batch_size]\n",
        "            y = targets[i * batch_size: (i + 1) * batch_size]\n",
        "            yield (x, y)\n",
        "'''\n",
        "for epoch in range(0, 100):  # adjust the size of epochs\n",
        "\n",
        "    # training method\n",
        "    for i in range(10, 12):  # 0 can be changed to the file wanted to be started\n",
        "\n",
        "        # mix files for reducing nan\n",
        "        # read one different file each time\n",
        "        train_x, train_y = array(i)\n",
        "        for j in range(1, 3):  # concatenate separately to avoid memory error\n",
        "            n = i + j\n",
        "            if (i+j) > 13:  # for the last few files\n",
        "                n = n - 14\n",
        "            x, y = array(n)\n",
        "            train_x = np.concatenate((train_x, x))\n",
        "            train_y = np.concatenate((train_y, y))\n",
        "            del x, y  # clean memory\n",
        "\n",
        "        # if exist load weights\n",
        "        if os.path.exists(weight_path):\n",
        "            model.load_weights(weight_path)\n",
        "\n",
        "        # compile model\n",
        "        optimizer = Adadelta()\n",
        "        model.compile(loss='mse', optimizer=optimizer, metrics=['accuracy'])\n",
        "\n",
        "        # add check point\n",
        "        checkpoint = ModelCheckpoint(weight_path, monitor='val_acc', verbose=1, save_best_only=True, mode='max')\n",
        "        callback_list = [checkpoint]  # only save the best model\n",
        "\n",
        "        # fit the model\n",
        "        model.fit(x=train_x, y=train_y, validation_split=0.05, batch_size=1000, epochs=10,\n",
        "                  callbacks=callback_list, verbose=1, shuffle=True)\n",
        "\n",
        "        # evaluate the model\n",
        "        loss, accuracy = model.evaluate(train_x, train_y, verbose=1)\n",
        "        print(loss, accuracy)\n",
        "\n",
        "        # save the model\n",
        "        if loss != 'nan':\n",
        "            model.save(model_path)\n",
        "\n",
        "        # clean memory for next file\n",
        "        del train_x, train_y, loss, accuracy\n",
        "        gc.collect()\n",
        "        "
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/scipy/io/wavfile.py:273: WavFileWarning: Chunk (non-data) not understood, skipping it.\n",
            "  WavFileWarning)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Train on 113122 samples, validate on 5954 samples\n",
            "Epoch 1/10\n",
            "113122/113122 [==============================] - 1459s 13ms/step - loss: 1629.7136 - acc: 0.5902 - val_loss: 1615.5409 - val_acc: 0.5632\n",
            "\n",
            "Epoch 00001: val_acc improved from -inf to 0.56315, saving model to model/weights_djent.best.hdf5\n",
            "Epoch 2/10\n",
            "113122/113122 [==============================] - 1476s 13ms/step - loss: 1579.4719 - acc: 0.5963 - val_loss: 1601.6049 - val_acc: 0.5682\n",
            "\n",
            "Epoch 00002: val_acc improved from 0.56315 to 0.56819, saving model to model/weights_djent.best.hdf5\n",
            "Epoch 3/10\n",
            "113122/113122 [==============================] - 1466s 13ms/step - loss: 1535.5893 - acc: 0.6046 - val_loss: 1591.6734 - val_acc: 0.5667\n",
            "\n",
            "Epoch 00003: val_acc did not improve from 0.56819\n",
            "Epoch 4/10\n",
            "113122/113122 [==============================] - 1451s 13ms/step - loss: 1506.8380 - acc: 0.6100 - val_loss: 1583.4408 - val_acc: 0.5712\n",
            "\n",
            "Epoch 00004: val_acc improved from 0.56819 to 0.57121, saving model to model/weights_djent.best.hdf5\n",
            "Epoch 5/10\n",
            " 77000/113122 [===================>..........] - ETA: 7:41 - loss: 1480.5554 - acc: 0.6161"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}